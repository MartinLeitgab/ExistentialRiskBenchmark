============================= test session starts ==============================
platform darwin -- Python 3.11.14, pytest-7.4.4, pluggy-1.6.0 -- /opt/anaconda3/envs/x-risk-bench/bin/python3.11
cachedir: .pytest_cache
rootdir: /Users/riccardocampanella/Documents/Benchmarks/ExistentialRiskBenchmark
configfile: pytest.ini
plugins: anyio-4.12.1, langsmith-0.6.4
collecting ... collected 13 items / 3 deselected / 10 selected

pipeline_a_scenarios/tests/test_llm_client.py::test_integration_marker PASSED
pipeline_a_scenarios/tests/test_llm_client.py::test_batch_submission[anthropic] [INFO] Saved anthropic batch ID â†’ /Users/riccardocampanella/Documents/Benchmarks/ExistentialRiskBenchmark/pipeline_a_scenarios/tests/batch_data/anthropic_batch_id.txt
[INFO] Submitted anthropic batch: msgbatch_01P5nrC8vD1diD9KTgohpoRP
PASSED
pipeline_a_scenarios/tests/test_llm_client.py::test_batch_submission[openai] [INFO] Saved openai batch ID â†’ /Users/riccardocampanella/Documents/Benchmarks/ExistentialRiskBenchmark/pipeline_a_scenarios/tests/batch_data/openai_batch_id.txt
[INFO] Submitted openai batch: batch_697c97e854c48190934c920ebfe523ea
PASSED
pipeline_a_scenarios/tests/test_llm_client.py::test_batch_submission[google] 
âš¡ Using PARALLEL single-shot for 2 prompts (bypass broken Batch API)

ğŸš€ PARALLEL MODE: Executing 2 prompts concurrently...

[DEBUG Google] Model: gemini-3-flash-preview
[DEBUG Google] Max output tokens: 8000 (requested: 1000)
[DEBUG Google] Temperature: 0.7
[DEBUG Google] Thinking config: None

[DEBUG Google] Model: gemini-3-flash-preview
[DEBUG Google] Max output tokens: 8000 (requested: 1000)
[DEBUG Google] Temperature: 0.7
[DEBUG Google] Thinking config: None
[DEBUG Google] Response candidates: 1
[DEBUG Google] Finish reason: FinishReason.STOP
[DEBUG Google] Content parts: 1
[DEBUG Google] Full text length: 164 chars
[DEBUG Google] Text preview: Quantum computers use **qubits** to process information in multiple states simultaneously, solving c
[DEBUG Google] Actual tokens - input: 10, output: 24
[DEBUG Google] Response candidates: 1
[DEBUG Google] Finish reason: FinishReason.STOP
[DEBUG Google] Content parts: 1
[DEBUG Google] Full text length: 70 chars
[DEBUG Google] Text preview: AI learns from **data patterns** to make **predictions** or decisions.
[DEBUG Google] Actual tokens - input: 9, output: 15
âœ… Parallel execution complete: 2 results
[INFO] Saved google batch ID â†’ /Users/riccardocampanella/Documents/Benchmarks/ExistentialRiskBenchmark/pipeline_a_scenarios/tests/batch_data/google_batch_id.txt
[INFO] Submitted google batch: parallel-1769773036
[INFO] Results ready immediately (parallel execution)
[INFO] Saved google batch results â†’ /Users/riccardocampanella/Documents/Benchmarks/ExistentialRiskBenchmark/pipeline_a_scenarios/tests/batch_results/google_batch_results.json
[INFO] Saved results to: /Users/riccardocampanella/Documents/Benchmarks/ExistentialRiskBenchmark/pipeline_a_scenarios/tests/batch_results/google_batch_results.json
PASSED
pipeline_a_scenarios/tests/test_llm_client.py::test_batch_wait_and_retrieve[anthropic] 
============================================================
RETRIEVING BATCH: msgbatch_01P5nrC8vD1diD9KTgohpoRP
============================================================

â³ Slow retrieval (Batch API - may take 15+ min)...
[INFO] Saved anthropic batch results â†’ /Users/riccardocampanella/Documents/Benchmarks/ExistentialRiskBenchmark/pipeline_a_scenarios/tests/batch_results/anthropic_batch_results.json

============================================================
âœ… SUCCESS: Retrieved 4 results
============================================================
PASSED
pipeline_a_scenarios/tests/test_llm_client.py::test_batch_wait_and_retrieve[openai] 
============================================================
RETRIEVING BATCH: batch_697c97e854c48190934c920ebfe523ea
============================================================

â³ Slow retrieval (Batch API - may take 15+ min)...
[DEBUG] OpenAI batch batch_697c97e854c48190934c920ebfe523ea initial status: in_progress
[DEBUG] Batch status changed: in_progress -> completed
[DEBUG] output_file_id: file-LoiFEjGMEBh1Emq22afqem
[DEBUG] error_file_id: None
[DEBUG] Request counts: total=2, completed=2, failed=0
[DEBUG] Retrieving output file: file-LoiFEjGMEBh1Emq22afqem
[DEBUG] Results: 2/2 successful
[INFO] Saved openai batch results â†’ /Users/riccardocampanella/Documents/Benchmarks/ExistentialRiskBenchmark/pipeline_a_scenarios/tests/batch_results/openai_batch_results.json

============================================================
âœ… SUCCESS: Retrieved 2 results
============================================================
PASSED
pipeline_a_scenarios/tests/test_llm_client.py::test_batch_wait_and_retrieve[google] 
============================================================
RETRIEVING BATCH: parallel-1769773036
============================================================

âš¡ Fast retrieval (parallel batch - results pre-computed)
[INFO] Loading pre-saved results from: /Users/riccardocampanella/Documents/Benchmarks/ExistentialRiskBenchmark/pipeline_a_scenarios/tests/batch_results/google_batch_results.json
[INFO] Saved google batch results â†’ /Users/riccardocampanella/Documents/Benchmarks/ExistentialRiskBenchmark/pipeline_a_scenarios/tests/batch_results/google_batch_results.json

============================================================
âœ… SUCCESS: Retrieved 2 results
============================================================
PASSED
pipeline_a_scenarios/tests/test_llm_client.py::test_cleanup_stuck_gemini_batch Cancelling stuck batch: parallel-1769773036
Could not cancel (may already be terminal): Invalid batch job name: parallel-1769773036.
PASSED
pipeline_a_scenarios/tests/test_llm_client.py::test_check_gemini_queue 
============================================================
GEMINI BATCH QUEUE STATUS
============================================================

ğŸ“¡ Fetching jobs from API...

ğŸ“Š SUMMARY:
  ğŸ• QUEUED (waiting):   0
  âš™ï¸  RUNNING:           0
  âœ… SUCCEEDED:          16
  âŒ FAILED/CANCELLED:   22
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  ğŸ“ˆ TOTAL:              39

ğŸš¨ ACTIVE (blocking new jobs): 0

ğŸ—‘ï¸  RECENT FAILED/CANCELLED (last 3):
   â€¢ batches/po8x0qdhe2zynnxgdpd7cuwgo5ppva5gvh1a (JOB_STATE_CANCELLED)
   â€¢ batches/g7grj07lssklxg3j7usydumfgq3l38rxlrs0 (JOB_STATE_CANCELLED)
   â€¢ batches/47zckdimawqk6f4wgr07es1pahn51yk81v4p (JOB_STATE_CANCELLED)

============================================================
Check complete. Queue depth: 0 active jobs
============================================================

PASSED
pipeline_a_scenarios/tests/test_llm_client.py::test_check_current_gemini_batch 
============================================================
CHECKING CURRENT BATCH STATUS
============================================================
ğŸ“„ Saved batch ID: parallel-1769773036

ğŸ” Fetching specific batch from API...
âŒ Error fetching batch: Invalid batch job name: parallel-1769773036.
   This usually means the batch ID is invalid or expired.

============================================================
QUEUE CONTEXT (for comparison)
============================================================
Total jobs in account: 39
Active jobs: 1

âš ï¸  WARNING: 1 other jobs are active!
   Your job may be stuck behind these in the queue.
   - batches/4o6zo395351wuajhda366848ho4zq55xp0c0

============================================================
PASSED

=============================== warnings summary ===============================
pipeline_a_scenarios/tests/test_llm_client.py:286
  /Users/riccardocampanella/Documents/Benchmarks/ExistentialRiskBenchmark/pipeline_a_scenarios/tests/test_llm_client.py:286: PytestUnknownMarkWarning: Unknown pytest.mark.veryslow - is this a typo?  You can register custom marks to avoid this warning - for details, see https://docs.pytest.org/en/stable/how-to/mark.html
    @pytest.mark.veryslow

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========== 10 passed, 3 deselected, 1 warning in 147.35s (0:02:27) ============
